---
title: "Discrete Data Analysis"
subtitle: "A Friendly Guide to Visualising Categorical Data for Machine Learning Practitioners"
author: "Julian Hatwell"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  beamer_presentation:
    colortheme: "seahorse"
    fonttheme: "structurebold"
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(warning = FALSE
              , message = FALSE
              , echo = FALSE
              )

library(knitr)
library(lattice)
library(ggplot2)
library(vcd)
library(vcdExtra)
library(ca)
library(randomForest)
library(mvtnorm)
library(ROSE)

# a couple of custom funcs for later
my_rose <- function(cm) {
  fourfold(cm
         , std = "all.max" # mimic a 4 way rose plot
         , conf_level = 0 # suppress confint tests, not useful here
         , color = c("#FF0000", "#000080") # need to force colours due to suppressing confint test
         )
}

# while we're at it, let's make a function to get the numeric statistics that we want from the conf mat
my_confmat_stats <- function(cm) {
  cf_stats <- list()
  for (i in 1:(dim(cm)[3])) {
    
    category <- dimnames(cm)[[3]][i]
    acc <- sum(diag(cm[ , , i]))/sum(cm[ , , i])
    
    # False Negative Rate FN / (TP + FN)
    fnr <- cm[2, 1, i]/sum(cm[ , 1, i])
    
    # False Ommision Rate FN / (TN + FN)
    fomr <- cm[2, 1, i]/sum(cm[2, , i])
    
    # collect
    cf_stats[[category]] <-
      c(acc=acc, fnr=fnr, fomr=fomr)
  }
  
  return(t(as.data.frame(cf_stats)))
}
```

# Introduction

## About Me

* Over 15 years in enterprise systems, management information systems and BI in the for-profit education sector, UK and Singapore
* MSc (Distinction) Business Intelligence, Birmingham City University. Master's Dissertation: *An Association Rules Based Method for Imputing Missing Likert Scale Data*
* Research to PhD (in progress): *Designing Explanation Systems for Decision Forests*, Data Analytics and Artifical Intelligence research group, Birmingham City University  

## Academic Analytics

* Academic Analytics
    * Typical BI analysis of sales (recruitment), financial, HR, operations, etc
    * Additionally, monitor, evaluate and optimise:
        * student life-cycle: from prospectus to alumni services
        * recruitment, including best match student-course
        * retention, minimising drop outs
        * on-time / on-target completion
        * progression to next level
        * student satisfaction, surveys
        * class room utilisation and exam planning
        * lecturer management, including externals
        
Not the same as

* Educational Data Mining
    * Analysis of pedogogical data (teaching and learning)
    * Student engagement data
    * Models of student understanding and learning
    * Topic modeling

## Credits

**Michael Friendly** is a pioneer in this field and has contributed to the development of modules and libraries for SAS and R.

Code examples based on material contained in the book [**Discrete Data Analysis with R: Visualization and Modeling Techniques for Categorical and Count Data**](https://www.crcpress.com/Discrete-Data-Analysis-with-R-Visualization-and-Modeling-Techniques-for/Friendly-Meyer/p/book/9781498725835) by Michael Friendly and David Meyer.

Shorter, valuable tutorial on these topics in the [vcd vignette](https://cran.r-project.org/web/packages/vcdExtra/vignettes/vcd-tutorial.pdf). Just type **vignette("vcd")** at the R console.

This **video** https://www.youtube.com/watch?v=qfNsoc7Tf60 is a much more in depth lecture, by Michael Friendly, on topics covered in the book.

# Exploring Data with Area Based Plots

## In 3D with Hair Colour, Eye Colour and Gender

592 stats students, self-categorised, University of Delaware, 1974

```{r describe_haireye, echo=FALSE}
hr_desc <- data.frame(No=1:3
                      , Name=c("Hair", "Eye", "Sex")
                      , Levels=c("Black, Brown, Red, Blond", "Brown, Hazel, Green, Blue", "Male, Female"))
kable(hr_desc)
```

## Hard to Parse Lots of Numbers

In 2-D, ignoring Sex for now

```{r HEC}
HEC <- HairEyeColor[, c("Brown", "Hazel", "Green", "Blue"), ]

haireye <- margin.table(HEC, 1:2)
structable(haireye)
```

## Naive Approach: Barplot

You're forced to favour one features over the other. Does hair colour depend on eye colour?

```{r not_intuitive, fig.width = 6, fig.height = 4}
haireye <- margin.table(HEC, 1:2)
barplot(haireye
        , ylab = "Count"
        , beside = TRUE
        , legend = TRUE)
```

## Naive Approach: Barplot

Or does eye colour depend on hair colour? We can see there is a relationship, but comparisons are not intuitive.

```{r not_intuitive2, fig.width = 6, fig.height = 5}
# pivot the other way
barplot(aperm(haireye, 2:1)
        , ylab = "Count"
        , beside = TRUE
        , legend = TRUE)
```

## Mosaic Plot - Hair Marginal Counts

```{r hr_marg_h}
mosaic(margin.table(haireye, 1)
       , shade = TRUE
       , main="Hair Marginal Totals"
       , labeling = labeling_values
       , value_type = "observed"
       , gp_text = gpar(fontface = 1)
       , rot_labels = c(top = -20))
```

## Mosaic Plot - Eye Marginal Counts

```{r hr_marg_e}
mosaic(margin.table(haireye, 2)
       , shade = TRUE
       , split_vertical = TRUE
       , main="Eye Marginal Totals"
       , labeling = labeling_values
       , value_type = "observed"
       , gp_text = gpar(fontface = 1)
       , rot_labels = c(top = -20))
```

## Mosaic Plot - Expected Counts

```{r hr_expected}
# visualising expected counts for independent features
expected = independence_table(haireye)

mosaic(expected
      , shade = TRUE
      , main="Expected frequencies"
      , labeling = labeling_values
      , value_type = "expected"
      , gp_text = gpar(fontface = 1))
```

## Mosaic Plot - Observed Counts

```{r hr_actuals}
mosaic(haireye
      , gp = shading_hcl # shade = TRUE
      , main="Actual frequencies"
      , labeling = labeling_values
      , value_type = "observed"
      , gp_text = gpar(fontface = 1))
```

## Mosaic Plot - Friendly Colour Scheme

```{r hr_actuals_friend}
# visualising actual counts for independent features
mosaic(haireye
      , gp = shading_Friendly # shade = TRUE
      , main="Actual frequencies"
      , labeling = labeling_values
      , value_type = "observed"
      , gp_text = gpar(fontface = 1))
```

## Previous + Sex Feature: Now in 3D

```{r hec}
# showing all three dimensions
mosaic(HEC
      , gp = shading_Friendly # shade = TRUE
      , main="Actual frequencies"
      , labeling = labeling_values
      , value_type = "observed"
      , gp_text = gpar(fontface = 1), rot_labels = c(right = -45)) 
```

## What You See Depends on the Pivot

```{r hec2}
# with a different pivot
mosaic(aperm(HEC, 3:1)
      , gp = shading_Friendly # shade = TRUE
      , main="Actual frequencies"
      , labeling = labeling_values
      , value_type = "observed"
      , gp_text = gpar(fontface = 1), rot_labels = c(right = -45)) 
```

## Mosaic Plot in 4D - Who Survived the Titanic?

```{r titanic_mosaic}
mosaic(aperm(Titanic, c(4, 2, 1, 3))
      , gp = shading_Friendly # shade = TRUE
      , main="Who Died and Who Survived the Titanic?"
      , labeling = labeling_values
      , value_type = "observed"
      , gp_text = gpar(fontface = 1), rot_labels = c(right = -45))
```

## Mosaic in n-D - Faceting

```{r titanic_cotab, echo=TRUE}
# note; all defaults except the shading!
cotabplot(Titanic, gp = shading_Friendly)
```

## Mosaic - Summary

The final plot didn't render nicely on these slides. Screen real-estate at run time was used more efficiently.

Important points:

* Mosaic plots; area $\propto$ cell count
* Fill colour by size of deviance residual
* Option for outline colour by deviance residual sign
* Scales well to 4-D
* At least a further 2-D can elevate up to facet
* Allows visual exploration of n-way interactions

# Clustering and Feature Engineering with Correspondence Analysis

## CA - PCA for Discrete Data

Correspondence Analysis is Machine Learning - a different model with different data.

Correspondence Analysis is a matrix decomposition - deterministic for same data.

## Audience Viewing Data

Audience viewing data from Neilsen Media Research for the week starting November 6, 1995

It is a 3-D array cross-tabulating the viewing figures for three networks, between 8-11pm, Monday to Friday. The features and their levels are as follows:

```{r describe_tv}
tv_desc <- data.frame(No=1:3
                      , Name=c("Day", "Time", "Network")
                      , Levels=c("Monday, Tuesday, Wednesday, Thursday, Friday", "8, 9, 10", "ABC, CBS, NBC"))
kable(tv_desc)
```

```{r tv_dataprep}
data("TV", package = "vcdExtra")
# The original data is collected in 15 minutes slices.

# Convert 3-D array to a frequency data frame
# This has a row for each cell of the array
# and a new column for the cell value
TV.df <- as.data.frame.table(TV)

# Convert it into hourly slices
levels(TV.df$Time) <- rep(c("8", "9", "10"), c(4,4,3))

# Convert frequency data back to 3-D array, now with just 3 time levels
TV3 <- xtabs(Freq~Day+Time+Network, TV.df)
```

## Multiple Correspondence Analysis

Multiple/Joint Correspondence Analysis analysis relationships between all features simultaneously.

```{r tv_3wayca, echo=TRUE}
# multiple CA - one line of code!
TV3.mca <- mjca(TV3)
```

## Multiple Correspondence Analysis Plot

```{r TV_mca_plot, fig.width=6, fig.height=5}
# the plot function uses all base R plot stuff
# but needs a bit of manipulation
cols <- c("blue", "black", "red")

# "blank plot"
res <- plot(TV3.mca, labels=0, pch='.', cex.lab=1.2)

# combine Dims, factor names and levels
coords <- data.frame(res$cols, TV3.mca$factors)

# save this for later
m_coords <- coords[ order(coords[,"factor"], -coords[,"Dim1"]), c("Dim1", "Dim2")]

# hard-coded from known number of levels
# day, time, network
nlev <- c(5,3,3)

# everything needs to be in semantic order
coords <- coords[ order(coords[,"factor"], coords[,"level"]), ]
# quick fix for ordering e.g. day of week, not alphabetical
coords$order <- c(5, 1, 4, 2, 3, 6, 7, 8, 11, 9, 10)
coords <- coords[order(coords[, "order"]), ]

# place the points with separate plot chars and colours
points(coords[,1:2], pch=rep(16:18, nlev), col=rep(cols, nlev), cex=1.2)

# place the text
pos <- c(1,4,3)
text(coords[,1:2], labels=coords$level, col=rep(cols, nlev), pos=rep(pos,nlev), cex=1.1, xpd=TRUE)

# join things in sequence
# lines(Dim2 ~ Dim1, data=coords, subset=factor=="Day", lty=1, lwd=1, col="blue")
# lines(Dim2 ~ Dim1, data=coords, subset=factor=="Time",  lty=1, lwd=1, col="red")

# add segements from the origin to points
nw <- subset(coords, factor=="Network")
segments(0, 0, nw[,"Dim1"], nw[, "Dim2"], col = "black", lwd = 1, lty = 3)

tm <- subset(coords, factor=="Time")
segments(0, 0, tm[,"Dim1"], tm[, "Dim2"], col = "red", lwd = 1, lty = 3)

dy <- subset(coords, factor=="Day")
segments(0, 0, dy[,"Dim1"], dy[, "Dim2"], col = "blue", lwd = 0.75, lty = 3)

# add a legend
legend("topright", legend=c("Day", "Network", "Time"),
       title="Factor", title.col="black",
       col=cols, text.col=cols, pch=16:18,
       bg="gray95")
```

## Multiple Correspondence Analysis - Findings

```{r mca_findings}
cat("All time slots")
margin.table(TV3, 1) # Day Marginal Totals
margin.table(TV3, c(1, 3)) # Day by Channel
```

***
```{r mca_findings2}
cat("All Days and Channels")
margin.table(TV3, 2) # Time Marginal Totals
cat("Thursday")
t(TV3[4,,]) # Thursday
```

## Simple Correspondence Analysis

```{r simple_ca, echo=TRUE}
# Flatten to 2-D by stacking time and day
TV3s <- as.matrix(structable(Network~Time+Day
                             , TV3))

# simple CA - one line of code!
TV3s.ca <- ca(TV3s)
```

## Simple Correspondence Analysis Plot

```{r TV.ca, fig.width=6, fig.height=5}
# Generate the plot
res <- plot(TV3s.ca)
# add some segments from the origin to make things clearer
segments(0, 0, res$cols[,1], res$cols[,2], col = "red", lwd = 1)
segments(0, 0, res$rows[,1], res$rows[,2], col = "blue", lwd = 0.5, lty = 3)
```

## Simple Correspondence Analysis Findings

```{r ca_findings}
cat("Monday")
t(TV3[1,,]) # Monday
cat("Every Day, 9pm, ABC")
TV3[,2,1] # Every day, 9pm, ABC
```

## CA Converts Categorical to Continuous

```{r TV_mca_plot2, fig.width=6, fig.height=5}
# "blank plot"
res <- plot(TV3.mca, labels=0, pch='.', cex.lab=1.2)

# combine Dims, factor names and levels
coords <- data.frame(res$cols, TV3.mca$factors)

# hard-coded from known number of levels
# day, time, network
nlev <- c(5,3,3)

# everything needs to be in semantic order
coords <- coords[ order(coords[,"factor"], coords[,"level"]), ]
# quick fix for ordering e.g. day of week, not alphabetical
coords$order <- c(5, 1, 4, 2, 3, 6, 7, 8, 11, 9, 10)
coords <- coords[order(coords[, "order"]), ]

# place the points with separate plot chars and colours
points(coords[1:5,1:2], pch=rep(16:18, nlev), col=rep(cols, nlev), cex=1.2)

# place the text
pos <- c(1,4,3)
text(coords[1:5,1:2], labels=c("Mon", "Tue", "Wed", "Thu", "Fri"), col=rep(cols, nlev), pos=rep(pos,nlev), cex=0.8, xpd=TRUE)

dy <- subset(coords, factor=="Day")
segments(dy[,"Dim1"], 0, dy[,"Dim1"], dy[, "Dim2"], lty=3, lwd=0.25, col="blue")
points(rep(0, length(dy[,"Dim1"]))~dy[,"Dim1"], col="blue")
segments(0, dy[,"Dim2"], dy[,"Dim1"], dy[, "Dim2"], lty=3, lwd=0.25, col="blue")
points(dy[,"Dim2"]~rep(0, length(dy[,"Dim2"])), col="blue")
```

## CA is Feature Engineering

Order is NOT arbitrary!

```{r res_coords}
m_coords
```

## Multiple Correspondence Analysis Plot in 4-D

```{r Titanic_mca}
# one more mca from a 4-D dataset
# this is simpler than it looks
# all the magic happens here
titanic.mca <- mjca(Titanic)

# saving the plot object supplies the coordinate positions
res <- plot(titanic.mca, labels=0, pch='.', cex.lab=1.2)

# extract factor names and levels
coords <- data.frame(res$cols, titanic.mca$factors)

# everything else is handling base R plotting stuff
cols <- c("blue", "red", "brown", "black")
nlev <- c(4,2,2,2)
points(coords[,1:2], pch=rep(16:19, nlev), col=rep(cols, nlev), cex=1.2)
pos <- c(3,1,1,3)
text(coords[,1:2], labels=coords$level, col=rep(cols, nlev), pos=rep(pos,nlev), cex=1.1, xpd=TRUE)
coords <- coords[ order(coords[,"factor"], coords[,"Dim1"]), ]
lines(Dim2 ~ Dim1, data=coords, subset=factor=="Class", lty=1, lwd=2, col="blue")
lines(Dim2 ~ Dim1, data=coords, subset=factor=="Sex",  lty=1, lwd=2, col="red")
lines(Dim2 ~ Dim1, data=coords, subset=factor=="Age",  lty=1, lwd=2, col="brown")
lines(Dim2 ~ Dim1, data=coords, subset=factor=="Survived",  lty=1, lwd=2, col="black")

legend("topleft", legend=c("Class", "Sex", "Age", "Survived"),
       title="Factor", title.col="black",
       col=cols, text.col=cols, pch=16:19,
       bg="gray95", cex=1.2)
```

## Correspondence Analysis - Summary

* CA is a very powerful technique based on matrix decomposition
* Offers additional perspective for exploring data
* Complex, non-parametric relationships are easily visualised can be explored
* Useful for reducing dimensions
* Converting categorical dimensions to continuous, while preserving information
* Sort data by CA dimension rather than natural ordering: [*Monday, Tuesday, Wednesday, Thursday, Friday*] $\nRightarrow$ [*1,2,3,4,5*]

# Slicing a Confusion Matrix by Important Features

## Exploring Binary Classification with Rose Plots

Special case $2 \times 2$ table. vcd package has a fourfold plot function which we can "trick" into behaving like a rose plot. This is a pie with fixed angles that represents counts on the radius: 

$$\mathit{count} \propto \sqrt{\mathit{area}}$$

vcd fourfold also automatically stratifies by a third variable! Just a snippet of code required to slice and dice our results.

## German Data Set - Credit Rating

A mix of discrete and continuous features. Target is rating: "bad" (30%) or "good" (70%)

A correctly classified bad rating - True Positive

A good classification of a true bad rating - False Negative

Very high cost per False Negative: Offer of cheap credit to customer with high risk of default!

Random Forest can only optimise 0-1 loss

A random forest is trained on 70% of the data (balanced by over-sampling) and evaluated on the remaining 30% (not balanced).

***

```{r german_randfor}
german <- read.csv(gzfile("german.csv.gz"))

set.seed(54321)
# train test split
idx <- sample(c(TRUE, FALSE)
              , size = nrow(german)
              , prob = c(0.7, 0.3)
              , replace = TRUE)

german_train <- german[idx, ]
german_test <- german[!idx, ]

# make the training data balanced in case over fits good
# originally 300 bad, 700 good
german_train <- ovun.sample(rating~.
                      , data=german_train
                      , method = "over"
                      , p = 0.5)$data

# restore the factor levels to correct order
german_train$rating <- relevel(german_train$rating, "bad") 

# train rf accepting all the defaults
german_rf <- randomForest(rating~., data=german_train)

# get the predictions on new data
german_preds <- predict(german_rf, newdata = german_test)

# confusion matrix
confmat <- with(german_test, table(german_preds, rating))
confmat
```

```{r main_cm_stats, echo=TRUE}
# accuracy
sum(diag(confmat))/sum(confmat)

# False Negative Rate FN / (TP + FN)
confmat[2, 1]/sum(confmat[, 1])

# False Ommision Rate FN / (TN + FN)
confmat[2, 1]/sum(confmat[2, ])
```

## Rose Plot Confusion Matrix

```{r main_cm_rose, echo=TRUE, fig.width=4.5, fig.height=4.5}
# std = "all.max" forces rose plot behaviour
fourfold(confmat, std = "all.max")
```

## Variable Importance Plots

Important in what way?

What does anyone do with this information?

```{r german_varimp}
varImpPlot(german_rf)
```

## Stratified Confusion Matrix Stats

```{r confmat_c_stats, echo=TRUE}
confmat_c <- with(german_test
                  , table(german_preds
                          , rating
                          , chk))

my_confmat_stats(confmat_c)
```

## Stratified Rose Plot - Confusion Matrix By chk

```{r confmat_c_rose}
# fourfold automatically stratifies by third (and fourth) level of n-dim table

my_rose(confmat_c)
```

## Scaling to Higher Dims: 10 Factor Levels

```{r rose_pps}
confmat_p <- with(german_test
                  , table(german_preds
                          , rating
                          , pps))

my_rose(confmat_p)
```

## Integer Valued Feature with 22 Unique Values

```{r my_rose_dur}
confmat_d <- with(german_test
                  , table(german_preds
                          , rating
                          , dur))
my_rose(confmat_d)
```

## 2 & 3-Way Interactions Possible - Log Odds Ratio

The odds ratio for the binary confusion matrix conveniently rearranges to:

$$\phi = \log\Big(\frac{\mathrm{TP} \times \mathrm{TN}}{\mathrm{FP} \times \mathrm{FN}}\Big)$$

Note, all the T in the numerator and all the F in the denominator. 

Very easy to interpret:

* $\phi \in \mathbb{R}^+$ is good but check confidence intervals
* $\phi > 3 \implies$ ratio of T:F $\approx 20:1$

Bonus: confidence intervals!

***

```{r lodds}
# higher dimensional
confmat_ce <- with(german_test
                   , table(german_preds
                           , rating
                           , chk
                           , emp))
plot(loddsratio(confmat_ce), confidence = FALSE)
```

***

```{r lodds2}
# higher dimensional
confmat_che <- with(german_test
                   , table(german_preds
                           , rating
                           , chk
                           , crhis
                           , emp))

plot(loddsratio(confmat_che), confidence = FALSE)
```


## Slicing a Confusion Matrix Summary

Important points:

* Use whenever you have suitable categorical variables
* Feature importance measure; a helpful guide but not essential.
* Drill into sources of bias
* Rose plots are intuitive for $2 \times 2$ tables and easy to produce
* vcd fourfold scales to tens of levels on one category
* Explore 2 and 3-way interactions with log odds ratios 
* Log odds ratios can't show the asymmetry in FNR and FPR

# While it's tempting to rush into using the latest tools and technologies, most Machine Learning is based on years of statistical research. That work continues to be relevant and applicable to today's challenges.

# End