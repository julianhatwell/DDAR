---
title: "Discrete Data Analysis for Machine Learning Practitioners, with Examples in R"
author: "Julian Hatwell"
date: "7 February 2019"
output: html_document
references:
- id: noack1950
  title: A class of random variables with discrete distributions.
  author:
  - family: Noack
    given: A.
  container-title: Annals of Mathematical Statistics
  volume: 21
  page: 127-132
  type: article-journal
  issued:
    year: 1950
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE
                      , message = FALSE
                      )

knitr::opts_template$set(
  fig.wide = list(fig.height = 4.5, fig.width = 8, fig.align='center')
  , fig.wideX = list(fig.height = 3, fig.width = 9, fig.align='center')
  , fig.relaxed = list(fig.height = 6, fig.width = 8, fig.align='center')
  , fig.tile = list(fig.height = 3, fig.width = 3, fig.align='center')
)
```

# Introduction

Note: This tutorial is based on material contained in the book [Discrete Data Analysis with R: Visualization and Modeling Techniques for Categorical and Count Data](https://www.crcpress.com/Discrete-Data-Analysis-with-R-Visualization-and-Modeling-Techniques-for/Friendly-Meyer/p/book/9781498725835) by Michael Friendly. Michael Friendly is a pioneer in this field and has contributed to the development of modules and libraries for SAS and R.

You can also find a much shorter tutorial on these topics in the long form help for the "vcd" package for R: [vcd vignette](https://cran.r-project.org/web/packages/vcdExtra/vignettes/vcd-tutorial.pdf)

## What is Discrete Data Analysis (DDA)?

Predictably, the term refers to analysis of discrete data! DDA is a set of tools and techniques that you would apply to achieve the same objectives as any quantitative statistical analysis but with these data types:

* Nominal
* Ordinal
* Counts / Frequencies

Nearly everyone with any background in stats has some experience of DDA! We have all performed a $\chi^2$ test for independence or for association. This is a hypothesis test involving categorical and count data. For example:

* $H_0 =$ In a random sample of the population, there is an equal chance of belonging to each category.

* $H_0 =$ In a random sample of the population, membership of any category in feature A is independent of membership of any category in feature B

In this workshop, we will go deeper than the basic $\chi^2$ to answer more detailed questions, and we will focus on visual tools that can reveal very interesting patterns in categorical data!

## What we won't cover

Bear in mind, this is just an overview! Our seminar session is just long enough for a taster and not much depth. We will focus on visual analytics and only introduce statistical tests where absolutely necessary.

We also won't cover explanatory and predictive modeling. This includes logistic regression which everyone will be familiar with. It falls squarely under both DDA and Machine Learning, and there are other techniques such as proportional odds for predictive modeling of ordinal classes, and loglinear models for categories, and glms for count data. This is all out of scope for today because there isn't time to review the underlying theories.

## Intended Audience

Data Analytics and Artificial Intelligence Research Group at Birmingham City University. A team of academics, post-doctoral researchers and doctoral candidates.

## Why Would You Be Interested in This Workshop?

The aim of this workshop is to provide a high-level overview of some less well known statistical tools that might help you with some very common tasks in your Machine Learning practice and research:

* Exploratory Data Analysis:
    * An essential, often overlooked, step prior to any analysis task
    * Assess a new dataset that contains several discrete features
    * Develop an intuition about the relationships between those features
* Rigorous and robust reporting of machine learning results:
    * Analysing frequency data: events, occurences, wait-times
    * Classification, especially multi-class problems involves categorical data
    * Sometimes a confusion matrix is not enough: ordinal classification is a special case

## Why Am I Presenting This Workshop?

This topic is one of the main reasons I became really interested in statistics in general. For me, *this was the way in!* Like many (most?) people, statistics was a bit alien, boring, mysterious, inpenetrable (all the above!), until I found a reason to actually apply it.

This specialist area of stats peaked my interest at two distinct points in my life:

1. My undergraduate degree in Microbiology - using techniques such as [Most Probable Number](https://en.wikipedia.org/wiki/Most_probable_number) to estimate the number of bacterial cells in a test tube. You certainly can't count them all!

2. My career as a data professional, included a stint in the tertiary education sector in Singapore. The kinds of questions I needed to answer were:
    * How do student grades compare across demographic groups (subject, gender, nationality, high school attended prior to entry, subjects and grades prior to entry)?
    * Is there a difference between the numbers of students who progress from undergrad to post-grad, depending on special offers?
    * Do different interventions improve the retention of at-risk students?
  
The types of questions we did *not* need to ask was "what is the average height/weight of students across different groups?" So it was obvious that regular descriptive and quantitative stats (i.e. the normal distribution) were not the right tools for this work.

# Preliminaries

If you prefer, you can simply follow along with the in person presentation but if you want to run the code chunks and see the results in the RStudio UI you need access to a machine with R+RStudio and the source code for this file.

To save time, make sure the following libraries are installed and loaded

```{r load_libs}
library(knitr)
library(vcd)
library(vcdExtra)
library(ca)
```

## Common distributions - the currency of DDA

There are several distributions to help with the most common DDA research questions. You can recognise them because a random variable from any of these can only be a natural number $\{ 0,1,2,\dots \}$:

* Binomial - how many heads in n coin tosses?
* Poisson - how many buses come every n minutes?
* Geometric - how many minutes do I wait before a bus comes?
* Negative Binomial - how many minutes waiting time between each bus?

These distributions are all special cases of the Power Series family of distributions [@noack1950]

There are lots of other more specialised distributions such as *hurdles* and *zero-inflated* models where a very high prevalence zero success events must be handled.

## Common DDA Tasks and Research Questions

This is a non-exhaustive list of common statistical tasks and questions that require specific DDA techniques. We will do short case studies on several of these that have shown to be useful in my Machine Learning research and practice. I hope there are some interesting nuggets here for you.

1. 1-way analysis and goodness of fit
    * $H_0 =$ My data fits a common, known distribution $\delta$, so I can use mathematical/analytical properties of $\delta$ for hypothesis tests.
    * Does my data fit a known distribution and if so, what are the parameters?
    * Does one random sample come from the same distribution as another?
2. 2-way analysis
    * 2 $\times$ 2 tables are a special case where a standard $\chi^2$ test is not recommended. What tools should I use instead?
    * $\chi^2$ tests on $n \times m$ tables can tell me that an association between features is present/absent, but if $n$ or $m$ are larger than 3-4, how can I find and test for complex relationships between so many combinations of categories?
    * Some of my categories are ordinal - what is the right test?
    * Classification problems with ordinal data - how can I quantify the agreement/disagreement between True and Predicted classes when there is a relationship between classes?
3. n-way analysis - Help! I just can't visualise and measure any of this in multiple dimensions.
    * Visualising n-way tables for exploratory analysis
    * Statistical hypothesis testing in n-way tables
    * Correspondence Analysis, a dimension reduction technique, like PCA and clustering for categorical variables!

## DDA in R and task specific visualisations

We don't have time develop a fluency for these tasks in this workshop but if this is something useful for you there are some "building block" skills that are worth spending time on:

1. The most useful data structures for DDA in R are not data frames, but n-dimensinal arrays, n-way tables, and matrices, sometimes data frames with a frequency column (for the count data).

1. Useful functions to get to know are table(), addmargins(), prop.table(), ftable(), xtabs(), xtable(), structable(), apply(), margin.table(), collapse.table(), aggregate(), aperm(), expand.dft(), subset(), using integer indexing to re-order categories and pivot dimensions

1. library "vcd" comes with its own plotting system. It is based on the grids package but is different from R base graphics, ggplot2 and lattice! It's easy to get started, but a bit of effort is required to learn all the tweaks for making publication ready graphical output. However, interpreting these area-based plots is really intuitive and it's worth the effort to learn so that results can be communicated succinctly (a picture tells a thousand words).

# Workshop

## Task 1: 1-way analysis (goodness of fit) for monitoring the frequency of events.

Count, or frequency data has many varied applications from event monitoring to text analysis. 1-way analysis uses distributions to compare different samples, and monitor for changes in event frequency.

For the first task we will look at two common distributions for count data and compare two samples.

The poisson distribution has just one paramater $\lambda$ for the rate, and both the mean and variance are equal to this parameter.

The negative binomial distribution has two parameters, rate and dispersion. The mean is equal to the rate, but the variance is always greater than the rate. It can appear as a right skewed poisson distribution and behaves as if a poisson distribution is being sampled over a period of time while the rate is non-constant.

However, data from these two distributions can look very similar. Identifying the best match and estimating the parameters is essential for accurate monitoring and reporting results.

### Case Study in Text Analysis: Marker word frequency

The following technique has been used to determine if documents are written by same or different authors. Distributions of marker words are fit to text that is known to come from one author. 

Any previously unseen text can then be tested against these known distributions to determine if they are written by the same author.

In this simple case study, we will use the Federalist dataset to analyse how many times the word 'may' appear per 200 word block. This is a marker word, that could help identify an author by how often they use it. Most studies of this kind would compare results across many such tests with different words.

Notes:

* The rate is occurences/200 words. 
* The expected count is the rate parameter $\times$ 200
* Plotting the square root transformation of the frequency provides more detail on the small numbers in the right tail of these charts. Log transformation would also work, but the rootogram visualisations use square root transformation by default. So, this is also applied manually where necessary for comparison.

```{r federalist}
data("Federalist", package = "vcd")
Federalist
sum(expand.dft(as.data.frame(Federalist))$nMay)
mean(expand.dft(as.data.frame(Federalist))$nMay)
var(expand.dft(as.data.frame(Federalist))$nMay)
```

```{r barplot_actual}
barplot(sqrt(Federalist)
        , main = "Number of text blocks by Number of Occurrences"
        , xlab = "Occurences of 'may'"
        , ylab = "Sqrt(Actual)"
        , col = "lightgreen"
)
```

A very naive way to estimate the poisson rate is just to take the mean.

```{r pois_naive_expected}
barplot(sqrt(200 * dpois(0:6, mean(expand.dft(as.data.frame(Federalist))$nMay)
))
        , main = "Number of text blocks by Number of Occurrences"
        , ylab = "sqrt(Naive Expected)"
        , xlab = "Occurences of 'may'"
        , col = "lightblue")
```

The actual data looks a bit like this poisson distribution with a $\lambda$ = mean of the sample. The discriptive stats showed a slightly higher variance but we don't know if this is significant. What's the right test to use here?

Use the goodfit function in the vcd package to fit both these distributions and estimate the parameters. The hanging rootogram charts are intuitive views of goodness of fit testing.

First, a poisson fit:

```{r fed_goodfit_pois}
Fed_fit0 <- goodfit(Federalist, type = "poisson")
# This will show the rate parameter, estimated by Maximum Likelihood
# This estimate is the same as the naive mean. This is to be expected when fitting a poisson.
unlist(Fed_fit0$par)

# This will show a Chi-Square Goodness of fit test against the expected values of a poisson distribution with this parameter
summary(Fed_fit0)

plot(Fed_fit0, type = "hanging", shade = TRUE)
```

The $\chi^2$ test shows a poor fit. We reject the null hypothesis that this is a sample from a poisson distributed random variable.

However, the plot makes it very clear what the problem is by showing how the variable diverges from the expectations. By hanging each bar top down from the expected distribution, we just need to look at how the bottoms of the bars miss the $y = 0$ reference line. The contribution of each bar to the $\chi^2$ statistic is also made clear by the shading.

Next a negative binomial fit:

```{r fed_goodfit_nbinom}
Fed_fit1 <- goodfit(Federalist, type = "nbinomial")
# This will show the rate and dispersion parameters, estimated by Maximum Likelihood
# prob is the rate. Here it's very close to the poisson mean
# size is the dispersion
unlist(Fed_fit1$par)
summary(Fed_fit1)

plot(Fed_fit1, type = "hanging", shade = TRUE)
```

The $\chi^2$ goodness of fit test is clear that the negative binomial distribution is the better choice. The plot speaks for itself now that the bars drop close to the $y = 0$ reference line. We have also estimated the parameters and can now use this distribution to assess a different text.

```{r nonFederalist}
nonFederalist <- structure(c(128L, 71L, 33L, 16L, 8L, 4L, 2L), .Dim = 7L, .Dimnames = structure(list(
    c("0", "1", "2", "3", "4", "5", "6")), .Names = "nMay"), class = "table")
nonFederalist
sum(expand.dft(as.data.frame(nonFederalist))$nMay)
mean(expand.dft(as.data.frame(nonFederalist))$nMay)
var(expand.dft(as.data.frame(nonFederalist))$nMay)
barplot(sqrt(nonFederalist)
        , main = "Number of text blocks by Number of Occurrences\n unseen text"
        , xlab = "Occurences of 'may'"
        , ylab = "Sqrt(Actual)"
        , col = "lightgreen"
)
```

The descriptive statistics are a bit larger, but is this significant? We can test it properly using the parameters from the goodfit object we just created:

```{r fit_nonfed}
# we pass in our fitted parameter list this time
Fed_fit3 <- goodfit(nonFederalist, type = "nbinomial", par = Fed_fit1$par)
summary(Fed_fit3)
plot(Fed_fit3, type = "hanging", shade = TRUE)
```

The word 'may' seems to appear more frequently in this text. The tail, with 3-6 occurences per 200 words is fatter, while blocks where this word is absent are fewer. The difference is significant. The $\chi^2$ result is evidence that this new text is not from the same distribution. 

### Talking points

* Does this test imply that the second text was not written by the same author?
* Text analysis has come a long way since these statistical techniques were developed. What are some newer, more sophisticated methods?
* Can you describe, in plain English, what is the implication of finding a poisson distributed word frequency? Why is the negative binomial a more natural distribution for word frequency? Hints:
    * Frequency is the number of times the word appeared per block of 200 words
    * Review the notes about these two distributions at the top of this task

## Task 2: 2-way analysis for reporting results for multi-class classification of ordinal variables.

In Machine Learning, the standard way to assess the quality of a supervised classification model is with a confusion matrix. This is a two way table comparing True and Predicted class labels. When classes are well balanced, it's easy to measure accuracy as $\frac{\sum(\textit{diagonal})}{\sum(\textit{total})}$. When they're out of balance, we use Cohen's $\kappa$ statistic to weight according to the occurence of each class in the data.

$\kappa$ = 1 means perfect agreement
$\kappa$ = 0 means no better than chance agreement

If you start with a class imbalance of 9:1 and and just predict a single class for any instance, you will get 90% accuracy but your $\kappa$ = 0 and that's not a good result!

But what if class labels are related to each other? Ordinal variables have a natural order, e.g. 

1. Strongly Agree
1. Agree
1. Neither Agree Nor Disagree
1. Disagree
1. Strongly Disagree

You want your model to predict as well as possible, but to make an off-by-one error isn't as bad as a full opposite error. Also, the classification distribution might be biased, different skew and kurtosis (flattened or polarised) by the predictive model.

If you can detect this kind of bias, you can correct it! This is where agreement plots can help. They are based on statistical techniques for comparing two raters, such as two doctors, two film critics, two wine critics etc, to valdiate ratings and analyse these kinds of bias.

### Case Study: Comparing inter-rater agreement between two neurologists assessing possible cases of multiple sclerosis.

In the following example we show how two sets of diagnoses for Multiple Sclerosis. The classes are ordinal ratings:

1. Certain
1. Probable
1. Possible
1. Doubtful

```{r MSP_2D}
# combine results from two cities (same raters are involved in all cases)
MSP <- margin.table(MSPatients, 1:2)
MSP
```

There are definitely some problems with this confusion matrix, but what exactly are they?

```{r MSP_accuracy}
# basic accuracy
sum(diag(MSP))/sum(MSP)
```

On the face of it, this looks like a terrible result! *Note, it's less than 0.5 but it's not worse than a random guess. There are 4 classes to choose from.* 

In this case study we can't say which is the "True" and which is "Predicted" because they're both independent raters, but for now let's assume that Winnipeg Neurologist is the "True" class.

```{r MSP_winntrue}
margin.table(MSP, 2)
```

This does look like some class imbalance. So we should assess this confusion matrix with $\kappa$

```{r MSP_kappa}
# cohen's kappa - the function in the vcd package provides weights and confidence intervals for this statistic
confint(Kappa(MSP, weights = "Fleiss-Cohen"))
```

Unweighted $\kappa$, the standard measure also shows that this is not a very good result. 

However, Fleiss-Cohen Weighted $\kappa$ favours near disagreements and taking this into account gives a much better result. So these raters are closer than it first appears.

We should use an agreement plot to look at how off-by-n errors and other bias could be affecting the outcome.

```{r MSP_agreement}
op <- par(mar = c(4, 3, 4, 1) + .1)
B <- agreementplot(MSP
                   , main = "MS Patient Ratings"
                   , xlab_rot = -20
)
par(op)
```

This plot needs some explaining. The rectangle outlines show the maximum possible agreement, given the marginal totals. The black rectangles indicate where there is full agreement between raters. The shaded area indicates the off by $n$ disagreement. *Note, if there were more categories, there would be more grades of shading*.

Ideally, we want the black area to fill the outline and it's not very close in this chart. However, the shaded area is coming much closer to filling the available space. 

```{r MSP_bangdi}
unlist(B)[1 : 2]
```

The Bangdiwala statistic is calculated based on the ratio of these shaded rectangles to the maximal outline. The large interval between the weighted and unweighted statistics shows that near disagreements are very common and if taken into account, indicate a much better alignment was indicated by a naive accuracy measure and even $\kappa$. 

There is another feature of this plot, which is the relationship of the rectangles to the diagonal line. This indicates how well aligned the raters are in using each category. In this chart, Winnipeg Neurologist seems to rate cases as "Certain" or "Probable" much more frequently than New Orleans Neurologist. Neither $\kappa$ nor Bangdiwala statistics can detect this. There are statistical tests available but this chart shows the problem directly.

With a little intervention, guided by this information, these two raters could become much better aligned. The same is true for ordinal classification models.

### Talking Points

* What other statistics, tools and techniques can you use to analyse a confusion matrix?
* Have you worked with, or can you think of an ordinal classification problem?

## Task 3: n-way analysis for exploring relationships between features

Exploratory Analysis is an essential, and frequently overlooked step as practitioners are often keen to use the latest tooling or are under pressure to deliver results. However, spending some time "getting to know" a new dataset and developing some intiution about important features and interactions will pay off by revealing a better strategy when it comes to modelling.

Now we will introduce the 'strucplot framework' that is the heart of the vcd package. This may be a new approach to structuring data and writing plotting routines compared to previous experience with R. However, it's worth the effort for the intuitive views of complex relationships in the data.

Everything is based on the simple concept that your features of interest are on the axes, and the area of plotted shapes is proportional to the count or frequency variable at the intersection of each category. Then, shading can be used in many ways to highlight important interactions. Today we will only use the mosaic plot, but this is the most useful as it scales well to 3 or even 4 dimensions.

### Case Study: Exploring the relationships between hair colour, eye colour and gender.

The HairEyeColor dataset ships with base R. It is a survey of 592 statistics students at the University of Delaware reported by Snee (1974). It is a 3-D array cross-tabulating the observations. The variables and their levels are as follows:

```{r describe_haireye, echo=FALSE}
hr_desc <- data.frame(No=1:3
                      , Name=c("Hair", "Eye", "Sex")
                      , Levels=c("Black, Brown, Red, Blond", "Brown, Blue, Hazel, Green", "Male, Female"))
kable(hr_desc)
```

For now, we will ignore the sex of each individual by collapsing the 3-D table down to 2-D, and re-order the table from dark to light eye colour (hair is already in that order).

```{r haireye}
haireye <- margin.table(HairEyeColor, 1:2)
haireye <- as.table(haireye[, c("Brown", "Hazel", "Green", "Blue")])
haireye
```

Big numbers and small numbers stand out but the table still has 16 cells which is a lot to take in at once. It's not immediately obvious what the relationships are.

Let's recall our old friend the $\chi^2$ test, which compares actual counts to expected counts. 

```{r hr_chisq}
chisq.test(haireye)
```

The null hypothesis is rejected. There is a very significant difference between actual and expected counts. Obviously there is a relationship between these two features, but we still don't know what it is.

Expected counts is what we would get if there wereno interaction between the features. Here's what no interaction looks like:

```{r hr_expected}
expected = independence_table(haireye)
round(expected, 1)

mosaic(expected
      , shade = TRUE
      , main="Expected frequencies"
      , labeling = labeling_values
      , value_type = "expected"
      , gp_text = gpar(fontface = 1))
```

Note that the areas are proportional to the counts. The grid lines between the tiles are dead straight because each cell is proportional to the margin products.

Here's what actuals looks like:

```{r hr_actuals}
mosaic(haireye
      , gp = shading_Friendly # shade = TRUE
      , main="Actual frequencies"
      , labeling = labeling_values
      , value_type = "observed"
      , gp_text = gpar(fontface = 1)
      , rot_labels = c(top = -20))
```

Here the grid lines between cells are no longer continuous because the cells are out of proportion with the margin products. Bigger cells means that the combination is over-represented and smaller cells the reverse. 

The beauty of the mosaic chart is the use of shading to highlight the $\chi^2$ residuals. This shading scheme uses fills for statistically significant residuals but also adds an outline colour to highlight the sign of even the non-significant residuals. This is visually very powerful.

Let's take this up to all three dimensions now.

```{r hec}
HEC <- HairEyeColor[, c("Brown", "Hazel", "Green", "Blue"), ]
mosaic(HEC
      , gp = shading_Friendly # shade = TRUE
      , main="Actual frequencies"
      , labeling = labeling_values
      , value_type = "observed"
      , gp_text = gpar(fontface = 1), rot_labels = c(right = -45)) 
```

This type of plot is still fairly intuitive at 4 dimension. More is possible, but it's harder work to interpret them. However, there are several options, such as faceting (i.e a trellis plot) to stratify over a further dimension, and mosaics can be featured in a plot matrix or pairs plot, where each feature is compared $2 \times 2$ inset on a grid.

Loglinear modeling can be used to explore the patterns further and make statistical hypothesis tests about complex n-way relationships between features.

### Talking points

* Recall that we've made sure the categories are ordered light to dark. There is a very pronounced "opposite corner" relationship. What do you think that reveals (in plain English) and does this fit with intuition?
* Can you see a difference between males and females? If so, can you explain it? *Note, the original study was a self-reporting survey. The participants categorised themselves.*
* Why do the grid lines remain aligned horizontally between hair categories on both actual frequencies mosaics?
* If you discovered important relationships in categorical features during your exploratory phase, how would this affect your Machine Learning strategy?

## Task 4: Correspondence Analysis for dimension reduction and clustering

Most Machine Learning practitioners are familiar with the need for dimension reduction. It can radically simplify the whole process. Many dimension reduction techniques also double up as unsupervised learning methods i.e. clustering. This is useful as an end in itself, as well as being an additional tool for exploratory data analysis: multi-dimensional data is projected into 2-D which makes it intuitive to visualise.

If you already know something about Principle Components Analysis, this is similar, but not quite the same. Remember, in DDA, the "dimensions" we want to reduce or cluster are not only the features, but also the categories within each feature.

Correspondence Analysis is an advanced topic. In particular, the way you pivot the multi-dimensional array controls the analysis. This requires some understanding of the data manipulation and modeling techniques. For today, we will just focus on interpreting the plots.

### Case Study: Analysis of Audience Viewing Data

The audience viewing data from Neilsen Media Research for the week starting November 6, 1995

It is a 3-D array cross-tabulating the viewing figures for three networks, between 8-11pm, Monday to Friday. The variables and their levels are as follows:

```{r describe_tv, echo=FALSE}
tv_desc <- data.frame(No=1:3
                      , Name=c("Day", "Time", "Network")
                      , Levels=c("Monday, Tuesday, Wednesday, Thursday, Friday", "8, 9, 10", "ABC, CBS, NBC"))
kable(tv_desc)
```

```{r tv_dataprep}
data("TV", package = "vcdExtra")
# The original data is collected in 15 minutes slices.

# Convert 3-D array to a frequency data frame
# This has a row for each cell of the array
# and a new column for the cell value
TV.df <- as.data.frame.table(TV)

# Convert it into hourly slices
levels(TV.df$Time) <- rep(c("8", "9", "10"), c(4,4,3))

# Convert frequency data back to 3-D array, now with just 3 time level
TV3 <- xtabs(Freq~Day+Time+Network, TV.df)
```

```{r tv_3wayca}
# create a multiple correspondence analysis
TV3.mca <- mjca(TV3)

# the plot function uses all base R plot stuff
# but needs a bit of manipulation
cols <- c("blue", "black", "red")

# "blank plot"
res <- plot(TV3.mca, labels=0, pch='.', cex.lab=1.2)

# combine Dims, factor names and levels
coords <- data.frame(res$cols, TV3.mca$factors)

# hard-coded from known number of levels
# day, time, network
nlev <- c(5,3,3)

# everything needs to be in semantic order
coords <- coords[ order(coords[,"factor"], coords[,"level"]), ]
# quick fix for ordering
coords$order <- c(5, 1, 4, 2, 3, 6, 7, 8, 11, 9, 10)
coords <- coords[order(coords[, "order"]), ]

# place the points
points(coords[,1:2], pch=rep(16:18, nlev), col=rep(cols, nlev), cex=1.2)

# place the text
pos <- c(1,4,3)
text(coords[,1:2], labels=coords$level, col=rep(cols, nlev), pos=rep(pos,nlev), cex=1.1, xpd=TRUE)

# join things in sequence
lines(Dim2 ~ Dim1, data=coords, subset=factor=="Day", lty=1, lwd=1, col="blue")
lines(Dim2 ~ Dim1, data=coords, subset=factor=="Time",  lty=1, lwd=1, col="red")

# add segement from the origin to channels
nw <- subset(coords, factor=="Network")
segments(0, 0, nw[,"Dim1"], nw[, "Dim2"], col = "black", lwd = 0.5, lty = 3)

# add a legend
legend("topright", legend=c("Day", "Time", "Network"),
       title="Factor", title.col="black",
       col=cols, text.col=cols, pch=16:18,
       bg="gray95")
```

This plot reveals an incredibly strong effect relating NBC and Thursday nights. Viewing figures are at their highest by far compared to any night of the week and most people were watching Friends, Seinfeld and ER, three of the most popular shows of that [era](https://en.wikipedia.org/wiki/1995%E2%80%9396_United_States_network_television_schedule). NBC has viewing figures that are comparable to the others most other nights.

The 9pm slot clusters closer to the other four days than 8pm and 10pm. It turns out that more people are watching TV at 9pm throughout the week than other times. The effect is nothing like as strong, but still enough that NBC audience numbers dipped a little for that time slot, even on Thursday.

CBS had the lion's share on Monday and ABC has Wednesday and Friday, with the audience split evenly on Tuesday.

These complex interactions would be much harder to understand on a mosaic plot.

```{r tv_mosaic}
# pivoting the dimensions for a clearer view
mosaic(xtabs(Freq~Network+Day+Time, TV.df)
       , gp = shading_Friendly # shade = TRUE
       #, rot_labels = c(top = -20)
       )
```

# Summary

In our rush to use the latest developments and technologies in Machine Learning and Deep Learning, it's easy forget that none of it would be possible without pioneering statistical research of previous decades. Most of this work is still relevant to everyday problems in Machine Learning practice, and it's always worth a review of the foundations before reaching for the newest tools.

* We have demonstrated some classical statistical techniques that are not usually included in a basic stats curriculum.
* We have discussed how these techniques can be applied to tasks that are typical in many practical Machine Learning projects.
    * Text/Event Analysis
    * Comparing True and Predicted Classes for Ordinal data
    * Exploratory Data Analyisis
    * Dimension Reduction and Clustering
* We have shown that simple R functions and visual analyses are extremely powerful and you don't have to dive deep into stats to get the benefits.
* Nearly all the resources you need to take the next steps yourself are available in the "vcd" and "vcdExtra" package for R, R~/vignette("vcd")
* If you want to go much, much deeper with these topics, [Michael Friendly's book](https://www.crcpress.com/Discrete-Data-Analysis-with-R-Visualization-and-Modeling-Techniques-for/Friendly-Meyer/p/book/9781498725835) pulls together decades of research in this field and is a one-stop reference for this branch of statistics.

# References